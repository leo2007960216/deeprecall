"""DeepRecall CLI -- command-line interface for serving, querying, and ingesting."""

from __future__ import annotations

import os
import sys

import click
from dotenv import load_dotenv

load_dotenv()


@click.group()
@click.version_option(version="0.1.0", prog_name="deeprecall")
def main() -> None:
    """DeepRecall -- Recursive reasoning over your data."""
    pass


@main.command()
@click.option(
    "--vectorstore",
    type=click.Choice(["chroma", "milvus", "qdrant", "pinecone"]),
    default="chroma",
    help="Vector store backend.",
)
@click.option("--collection", default="deeprecall", help="Collection/index name.")
@click.option("--persist-dir", default=None, help="Persist directory (ChromaDB only).")
@click.option("--host", default="0.0.0.0", help="Server host.")
@click.option("--port", default=8000, type=int, help="Server port.")
@click.option("--backend", default="openai", help="LLM backend.")
@click.option("--model", default="gpt-4o-mini", help="LLM model name.")
def serve(
    vectorstore: str,
    collection: str,
    persist_dir: str | None,
    host: str,
    port: int,
    backend: str,
    model: str,
) -> None:
    """Start the OpenAI-compatible API server."""
    try:
        import uvicorn
    except ImportError:
        click.echo("Error: uvicorn required. Install with: pip install deeprecall[server]")
        sys.exit(1)

    engine = _build_engine(vectorstore, collection, persist_dir, backend, model)

    from deeprecall.adapters.openai_server import create_app

    app = create_app(engine)
    click.echo(f"Starting DeepRecall server on {host}:{port}")
    click.echo(f"  Vector store: {vectorstore} ({collection})")
    click.echo(f"  Backend: {backend}/{model}")
    click.echo(f"  Docs: http://{host}:{port}/docs")
    uvicorn.run(app, host=host, port=port)


@main.command()
@click.argument("query_text")
@click.option(
    "--vectorstore",
    type=click.Choice(["chroma", "milvus", "qdrant", "pinecone"]),
    default="chroma",
    help="Vector store backend.",
)
@click.option("--collection", default="deeprecall", help="Collection/index name.")
@click.option("--persist-dir", default=None, help="Persist directory (ChromaDB only).")
@click.option("--backend", default="openai", help="LLM backend.")
@click.option("--model", default="gpt-4o-mini", help="LLM model name.")
@click.option("--verbose", is_flag=True, help="Show detailed output.")
def query(
    query_text: str,
    vectorstore: str,
    collection: str,
    persist_dir: str | None,
    backend: str,
    model: str,
    verbose: bool,
) -> None:
    """Query documents with recursive reasoning."""
    engine = _build_engine(vectorstore, collection, persist_dir, backend, model, verbose=verbose)
    result = engine.query(query_text)

    click.echo(f"\nAnswer: {result.answer}")
    click.echo(f"\nSources: {len(result.sources)}")
    click.echo(f"Time: {result.execution_time:.2f}s")
    click.echo(f"LLM calls: {result.usage.total_calls}")


@main.command()
@click.option("--path", required=True, type=click.Path(exists=True), help="Path to documents.")
@click.option(
    "--vectorstore",
    type=click.Choice(["chroma", "milvus", "qdrant", "pinecone"]),
    default="chroma",
    help="Vector store backend.",
)
@click.option("--collection", default="deeprecall", help="Collection/index name.")
@click.option("--persist-dir", default="./chroma_db", help="Persist directory (ChromaDB only).")
def ingest(
    path: str,
    vectorstore: str,
    collection: str,
    persist_dir: str,
) -> None:
    """Ingest documents from a file or directory into the vector store."""
    store = _build_vectorstore(vectorstore, collection, persist_dir)

    documents: list[str] = []
    metadatas: list[dict] = []

    if os.path.isfile(path):
        files = [path]
    else:
        files = [
            os.path.join(path, f)
            for f in os.listdir(path)
            if f.endswith((".txt", ".md", ".py", ".json", ".csv"))
        ]

    for filepath in files:
        with open(filepath) as f:
            content = f.read()
        documents.append(content)
        metadatas.append({"source": filepath, "filename": os.path.basename(filepath)})

    if not documents:
        click.echo("No documents found.")
        return

    ids = store.add_documents(documents=documents, metadatas=metadatas)
    click.echo(f"Ingested {len(ids)} documents into {vectorstore}/{collection}")
    click.echo(f"Total documents: {store.count()}")


def _build_vectorstore(
    vectorstore: str, collection: str, persist_dir: str | None = None
) -> BaseVectorStore:  # noqa: F821
    """Build a vector store from CLI args."""
    if vectorstore == "chroma":
        from deeprecall.vectorstores.chroma import ChromaStore

        return ChromaStore(collection_name=collection, persist_directory=persist_dir)
    elif vectorstore == "milvus":
        from deeprecall.vectorstores.milvus import MilvusStore

        return MilvusStore(collection_name=collection)
    elif vectorstore == "qdrant":
        from deeprecall.vectorstores.qdrant import QdrantStore

        return QdrantStore(collection_name=collection)
    elif vectorstore == "pinecone":
        from deeprecall.vectorstores.pinecone import PineconeStore

        return PineconeStore(index_name=collection)
    else:
        raise ValueError(f"Unknown vectorstore: {vectorstore}")


def _build_engine(
    vectorstore: str,
    collection: str,
    persist_dir: str | None,
    backend: str = "openai",
    model: str = "gpt-4o-mini",
    verbose: bool = False,
) -> DeepRecallEngine:  # noqa: F821
    from deeprecall.core.engine import DeepRecallEngine

    store = _build_vectorstore(vectorstore, collection, persist_dir)
    return DeepRecallEngine(
        vectorstore=store,
        backend=backend,
        backend_kwargs={
            "model_name": model,
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
        verbose=verbose,
    )


if __name__ == "__main__":
    main()
